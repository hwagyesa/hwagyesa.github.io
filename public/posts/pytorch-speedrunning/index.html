<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Pytorch Speedrunning | hwagyesa</title>
<meta name=keywords content="research"><meta name=description content="a weak coder focusing on the wrong aspects :')"><meta name=author content="hwagyesa"><link rel=canonical href=http://localhost:1313/posts/pytorch-speedrunning/><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/pytorch-speedrunning/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><meta property="og:title" content="Pytorch Speedrunning"><meta property="og:description" content="a weak coder focusing on the wrong aspects :')"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/pytorch-speedrunning/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-10-09T16:48:20-07:00"><meta property="article:modified_time" content="2024-10-09T16:48:20-07:00"><meta property="og:site_name" content="hwagyesa"><meta name=twitter:card content="summary"><meta name=twitter:title content="Pytorch Speedrunning"><meta name=twitter:description content="a weak coder focusing on the wrong aspects :')"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Pytorch Speedrunning","item":"http://localhost:1313/posts/pytorch-speedrunning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Pytorch Speedrunning","name":"Pytorch Speedrunning","description":"a weak coder focusing on the wrong aspects :')","keywords":["research"],"articleBody":"Rationale I don‚Äôt think I‚Äôm that bad at coding, but I‚Äôm certainly much slower than I‚Äôd like to be. I did not really learn pytorch until 2022, using a mixture of adhoc numpy and fast (but who cares) Matlab before that. For preparing for random ML coding questions in interviews (and, in some useful sense, I think, improving my general ability!), I‚Äôm trying to be a bit strategic and deliberate in my preparation. In this post I‚Äôll record some notes for my own benefit on how a bog-standard data-train-eval loop in pytorch can be blocked up into recognizable segments and rolled out quickly from scratch. I am sure this is uninteresting to every good ML experimenter out there; in writing it out this way for myself, I‚Äôm hoping it will be a useful preparation reference, as well as something that reduces my barrier to quick experimentation in new problem areas üòÑ\nThe exercise: linear regression We‚Äôll spin this out implementing an extremely simple linear regression problem in pytorch. Something like the following data model: $$ \\boldsymbol y = \\boldsymbol X \\boldsymbol \\beta_o + \\sigma \\boldsymbol g,$$ with $n$ observations, $d$ dimensions, noise standard deviation $\\sigma \u003e 0$, and everything i.i.d. $\\mathcal N(0, 1)$. In the ‚Äòclassical‚Äô regime where $n \\geq d$, we have as usual that the problem $$ \\min_{\\boldsymbol \\beta} \\frac{1}{2n} \\|\\boldsymbol y - \\boldsymbol X \\boldsymbol \\beta\\|_2^2$$ is solved in our random model (almost surely) by $$ \\boldsymbol \\beta_{\\star} = (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^\\top \\boldsymbol y.$$ It is not too hard to prove further that $\\| \\boldsymbol \\beta_{\\star} - \\boldsymbol \\beta_o \\|_2 \\lesssim \\sqrt{\\sigma^2 d / n}$ with overwhelming probability. So everything should be well-posed when we implement the model and test with a sample complexity of about $\\sigma^2 d$.\nFor this data model, we will:\nCode up the training loop. Code up the dataset. Code up the model. Add in eval (with wandb; maybe Hydra). I‚Äôll put notes on these sub-components in the sections below.\nTraining loop For interview coding, it seems to make sense to start by writing out the ‚Äòskeleton‚Äô of the training loop, then filling in the implementation-specific details: the model, the data, and different eval components necessary. This seems like a good structure to use as long as one has enough familiarity with pytorch to know how to start abstractly.\nThe following basic structure makes sense to me as a starting point:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def train( learning_rate = 1e-3, weight_decay = 0., batch_size = 32, num_samples_val = ..., **kwargs ): # TODO: # 1. logging (wandb) # ... # 2. get data train_dataloader = DataLoader( train_dataset, batch_size=batch_size, drop_last=False, shuffle=True ) val_dataloader = DataLoader( val_dataset, drop_last=False, shuffle=True, batch_size=num_samples_val ) # 3. get model model = ... # 4. training loop: loss function, optimizer, epoch and batch loop, batch processing. loss_fn = MSELoss(reduction=\"mean\") optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay) model.train() for epoch in range(num_epochs): for batch, (X, Y) in enumerate( train_dataloader ): loss = process_batch(model, X, Y, loss_fn, optimizer) # log stuff to wandb. # 5. Tests and logging. model.eval() # ... with torch.no_grad(): for X, Y in val_dataloader: loss = process_batch(model, X, Y, loss_fn, optimizer, training=False) print(f\"validation loss: {loss.detach()}\") sleep(1) For writing this out from nothing, one just needs to have an abstract sense of what one is aiming to do. We‚Äôll be training some model with some kind of gradient descent, and we might as well just use Adam; the objective is going to be some loss function which we should be able to identify once we have understood the problem we‚Äôre trying to solve at a modeling level. The ‚Äúthinking‚Äù tasks at this level thus seem to be:\nWhat‚Äôs the loss function? What are the parameters I need to pass in? Here I‚Äôve already written out some of the parameters we‚Äôll need: optimizer parameters, and some parameters associated to the dataset and dataloader. We‚Äôll eventually need more dataset parameters, possibly, as well as model parameters. In general, we might also have parameters associated to logging or the loss function. These can be added in as the other sections of the loop skeleton are filled out. What ‚Äúwork‚Äù do I need to do in the main loop, encapsulated in process_batch here? This function can take a pretty simple skeleton form: 1 2 3 4 5 6 7 8 9 def process_batch(model, X, Y, loss_fn, optimizer, training=True): preds = model(X) loss = loss_fn(preds, Y) if training: loss.backward() optimizer.step() optimizer.zero_grad() return loss.detach() We use the model to make predictions, then evaluate the loss. We accumulate gradients with backward, then step the optimizer and zero gradients.\nNOTE: it‚Äôs helpful here to add some asserts to debug your tensor shapes! Even on this simple linear regression problem, I spent some amount of time debugging a case where my predictions variable was (B, 1)-shaped but my targets variable was (B,)-shaped and broadcasting messed up the loss.\nDataset component The thing to remember here is just that a Dataloader (from torch.utils.data) wraps a Dataset object (from the same module). For this simple problem, we can create our own Dataset subclass (we‚Äôll do this momentarily). Remember the following for the dataloader:\nSet the batch size here. If we‚Äôre using CUDA, we can add the argument pin_memory=True and also use the non_blocking=True option in our dataset.to(device) calls that move data to the GPU. This can help performance. The other arguments to the dataloader are probably not essential. Implementing a dataset for our random linear regression model Here‚Äôs code for implementing the dataset, with ‚Äúthought organization‚Äù to follow.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class RandomDataset(Dataset): def __init__( self, data_dim: int, num_samples: int, ground_truth: torch.nn.Linear | None = None, device: torch.device = torch.device(\"cpu\"), noise_std: float = 0.1, transform: Callable | None = None, # target_transform: Callable | None = None, ) -\u003e None: super().__init__() self.device = device self.data = torch.randn( (num_samples, data_dim), device=device, dtype=torch.float32 ) self.noise_std = noise_std if ground_truth is not None: self.ground_truth = ground_truth # can't clone a pytorch module; need to deepcopy or manually else: self.ground_truth = torch.nn.Linear( data_dim, 1, device=device, dtype=torch.float32 ) torch.nn.init.normal_(self.ground_truth.weight) torch.nn.init.zeros_(self.ground_truth.bias) for parameter in self.ground_truth.parameters(): parameter.requires_grad = False with torch.no_grad(): self.targets = self.ground_truth(self.data) + self.noise_std * torch.randn( (num_samples, 1), device=device, dtype=torch.float32 ) self.transform = transform def __len__(self) -\u003e int: return self.data.shape[0] def __getitem__(self, idx) -\u003e tuple[torch.Tensor, torch.Tensor]: data = self.data[idx, :] if self.transform is not None: data = self.transform(data) return (data, self.targets[idx]) High-level points:\nWe subclass torch.utils.data.Dataset when we make our own dataset. A torch.utils.data.Dataset needs to implement __init__, __len__, and __getitem__. These are all instance methods; __getitem__ also takes an index (0 to __len__() - 1) and returns a tuple containing the indexed sample and its target/label. These should each be 1D tensors. For writing the code, we can sort of just write the __init__ function and think about what we need to set up for the data, then pretty easily do the other two functions. Set device and dtype for things. To make things comparable, even though it‚Äôs most natural to implement the model‚Äôs ground truth parameters with inline matrix operations, I used a torch.nn.Linear layer here (which will parallel what we do with the model below). For this, the arguments are (in_features, out_features, ...). For initialization, we use torch.nn.init; functions typically have a signature like ..._, and we just apply them to the module‚Äôs parameters. The linear layer‚Äôs parameters are called weight and bias. We also set the parameters‚Äô requires_grad to false, and include torch.no_grad() context when we generate the targets (ChatGPT says this tells pytorch not to track derived operations in the computational graph and saves memory). I‚Äôm including transform (for normalization and data augmentation) here just to be consistent with future stuff; will need this for more complicated implementations (e.g., CIFAR10). The model Here‚Äôs the model code, which is already familiar because of how we generated our dataset above.\n1 2 3 4 5 6 7 8 9 10 11 12 class LinearModel(torch.nn.Module): def __init__( self, data_dim: int, device: torch.device = torch.device(\"cpu\") ) -\u003e None: super().__init__() self.linear = Linear(data_dim, 1, device=device, dtype=torch.float32) torch.nn.init.normal_(self.linear.weight) torch.nn.init.zeros_(self.linear.bias) def forward(self, x) -\u003e torch.Tensor: return self.linear(x) Some short notes (reference the above):\nA pytorch model subclasses torch.nn.Module. It needs to implement __init__, where we set up its parameters and structure, and forward, an instance method that takes an input x and generates the layer output from it. Putting it together Here‚Äôs our overall train loop, callable from the command line.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def train( data_dim, num_samples_train, num_samples_val, device, noise_std=0.1, batch_size=32, num_epochs=128, **kwargs, ): # 1. logging (wandb) # We could do this with hydra. Chatgpt recommended a kwargs dict structure but hydra is better. Too much rewriting config table. # 2. get data train_dataset = RandomDataset( data_dim=data_dim, num_samples=num_samples_train, noise_std=noise_std, device=device, ) train_dataloader = DataLoader( train_dataset, batch_size=batch_size, drop_last=False, shuffle=True ) val_dataset = RandomDataset( data_dim=data_dim, num_samples=num_samples_val, noise_std=noise_std, ground_truth=train_dataset.ground_truth, ) val_dataloader = DataLoader( val_dataset, drop_last=False, shuffle=True, batch_size=num_samples_val ) # 3. get model model = LinearModel(data_dim, device) # 4. train loss_fn = MSELoss(reduction=\"mean\") optimizer = torch.optim.AdamW(model.parameters(), lr=1e-1, weight_decay=0.0) model.train() for epoch in range(num_epochs): for batch, (X, Y) in enumerate( train_dataloader ): # enumerate(iterable) gives us the batch idx. loss = process_batch(model, X, Y, loss_fn, optimizer) print(f\"batch {batch}, epoch {epoch}, loss: {loss.item()}\") # 5. Tests and logging. model.eval() print( f\"parameter error: {torch.sum((model.linear.weight - train_dataset.ground_truth.weight)**2)}\" ) with torch.no_grad(): for X, Y in val_dataloader: loss = process_batch(model, X, Y, loss_fn, optimizer, training=False) print(f\"validation loss: {loss.detach()}\") sleep(1) def process_batch(model, X, Y, loss_fn, optimizer, training=True): preds = model(X) loss = loss_fn(preds, Y) if training: loss.backward() optimizer.step() optimizer.zero_grad() return loss.detach() if __name__ == \"__main__\": num_samples_train = 128 config = { \"data_dim\": 16, \"num_samples_train\": num_samples_train, \"num_samples_val\": 128, \"noise_std\": 0.0, \"batch_size\": num_samples_train, \"num_epochs\": 2**10, \"device\": torch.device(\"cpu\"), } train(**config) We can write all this code in a bottom-up format without losing too much momentum!\nSome concluding thoughts I didn‚Äôt include much logging code above. It would be best to do this with wandb and hydra, but I guess there are not parameters to do this in an interview. The code above is not particularly optimized. ChatGPT gives some recommendations about enabling mixed-precision training (seems to be mild overhead) and implementing incremental logging. It would be fun to implement this exercise in a distributed training setting, as well. For doing more complex stuff, it probably makes sense to think about a way to write incremental tests (e.g., test the dataset is doing the right thing, test the model is doing the right thing) as one goes. Otherwise when something breaks it is a little bit annoying to drill down on what it is in this kind of ‚Äúbottom up‚Äù implementation scheme. ","wordCount":"1902","inLanguage":"en","datePublished":"2024-10-09T16:48:20-07:00","dateModified":"2024-10-09T16:48:20-07:00","author":{"@type":"Person","name":"hwagyesa"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/pytorch-speedrunning/"},"publisher":{"@type":"Organization","name":"hwagyesa","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=http://localhost:1313/posts/ title=posts><span>posts</span></a></li><li><a href=http://localhost:1313/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;¬ª&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Pytorch Speedrunning</h1><div class=post-description>a weak coder focusing on the wrong aspects :')</div><div class=post-meta><span title='2024-10-09 16:48:20 -0700 PDT'>October 9, 2024</span>&nbsp;¬∑&nbsp;hwagyesa&nbsp;|&nbsp;<a href=https://github.com/hwagyesa/hwagyesa.github.io/blob/main/content/posts/pytorch-speedrunning.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#rationale aria-label=Rationale>Rationale</a></li><li><a href=#the-exercise-linear-regression aria-label="The exercise: linear regression">The exercise: linear regression</a><ul><li><a href=#training-loop aria-label="Training loop">Training loop</a></li><li><a href=#dataset-component aria-label="Dataset component">Dataset component</a><ul><li><a href=#implementing-a-dataset-for-our-random-linear-regression-model aria-label="Implementing a dataset for our random linear regression model">Implementing a dataset for our random linear regression model</a></li></ul></li><li><a href=#the-model aria-label="The model">The model</a></li><li><a href=#putting-it-together aria-label="Putting it together">Putting it together</a></li></ul></li><li><a href=#some-concluding-thoughts aria-label="Some concluding thoughts">Some concluding thoughts</a></li></ul></div></details></div><div class=post-content><h1 id=rationale>Rationale<a hidden class=anchor aria-hidden=true href=#rationale>#</a></h1><p>I don&rsquo;t think I&rsquo;m <em>that</em> bad at coding, but I&rsquo;m certainly much slower than I&rsquo;d
like to be. I did not really learn pytorch until 2022, using a mixture of adhoc
numpy and fast (but who cares) Matlab before that. For preparing for random ML
coding questions in interviews (and, in some useful sense, I think, improving my
general ability!), I&rsquo;m trying to be a bit strategic and deliberate in my preparation.
In this post I&rsquo;ll record some notes for my own benefit on how a bog-standard
data-train-eval loop in pytorch can be blocked up into recognizable segments and
rolled out quickly from scratch. I am sure this is uninteresting to every good
ML experimenter out there; in writing it out this way for myself, I&rsquo;m hoping it
will be a useful preparation reference, as well as something that reduces my
barrier to quick experimentation in new problem areas üòÑ</p><h1 id=the-exercise-linear-regression>The exercise: linear regression<a hidden class=anchor aria-hidden=true href=#the-exercise-linear-regression>#</a></h1><p>We&rsquo;ll spin this out implementing an extremely simple linear regression problem
in pytorch. Something like the following data model:</p>$$ \boldsymbol y = \boldsymbol X \boldsymbol \beta_o + \sigma \boldsymbol g,$$<p>with $n$ observations, $d$ dimensions, noise standard deviation $\sigma > 0$,
and everything i.i.d. $\mathcal N(0, 1)$.
In the &lsquo;classical&rsquo; regime where $n \geq d$, we have as usual that the problem</p>$$ \min_{\boldsymbol \beta} \frac{1}{2n} \|\boldsymbol y - \boldsymbol X \boldsymbol \beta\|_2^2$$<p>is solved in our random model (almost surely) by</p>$$ \boldsymbol \beta_{\star} = (\boldsymbol X^\top \boldsymbol X)^{-1} \boldsymbol X^\top \boldsymbol y.$$<p>It is not too hard to prove further that $\| \boldsymbol \beta_{\star} - \boldsymbol \beta_o \|_2 \lesssim \sqrt{\sigma^2 d / n}$ with overwhelming probability.
So everything should be well-posed when we implement the model and test with
a sample complexity of about $\sigma^2 d$.</p><p>For this data model, we will:</p><ol><li>Code up the training loop.</li><li>Code up the dataset.</li><li>Code up the model.</li><li>Add in eval (with wandb; maybe Hydra).</li></ol><p>I&rsquo;ll put notes on these sub-components in the sections below.</p><h2 id=training-loop>Training loop<a hidden class=anchor aria-hidden=true href=#training-loop>#</a></h2><p>For interview coding, it seems to make sense to start by writing out the
&lsquo;skeleton&rsquo; of the training loop, then filling in the implementation-specific
details: the model, the data, and different eval components necessary.
This seems like a good structure to use as long as one has enough familiarity
with pytorch to know how to start abstractly.</p><p>The following basic structure makes sense to me as a starting point:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>train</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>learning_rate</span> <span class=o>=</span> <span class=mf>1e-3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>weight_decay</span> <span class=o>=</span> <span class=mf>0.</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>batch_size</span> <span class=o>=</span> <span class=mi>32</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>num_samples_val</span> <span class=o>=</span> <span class=o>...</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=o>**</span><span class=n>kwargs</span>
</span></span><span class=line><span class=cl><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># TODO:</span>
</span></span><span class=line><span class=cl>    <span class=c1># 1. logging (wandb)</span>
</span></span><span class=line><span class=cl>    <span class=c1># ...</span>
</span></span><span class=line><span class=cl>    <span class=c1># 2. get data</span>
</span></span><span class=line><span class=cl>    <span class=n>train_dataloader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>train_dataset</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>drop_last</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>val_dataloader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>val_dataset</span><span class=p>,</span> <span class=n>drop_last</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=n>num_samples_val</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 3. get model</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=o>...</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 4. training loop: loss function, optimizer, epoch and batch loop, batch processing.</span>
</span></span><span class=line><span class=cl>    <span class=n>loss_fn</span> <span class=o>=</span> <span class=n>MSELoss</span><span class=p>(</span><span class=n>reduction</span><span class=o>=</span><span class=s2>&#34;mean&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>AdamW</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>learning_rate</span><span class=p>,</span> <span class=n>weight_decay</span><span class=o>=</span><span class=n>weight_decay</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>batch</span><span class=p>,</span> <span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>train_dataloader</span>
</span></span><span class=line><span class=cl>        <span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>loss</span> <span class=o>=</span> <span class=n>process_batch</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>,</span> <span class=n>loss_fn</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=c1># log stuff to wandb.</span>
</span></span><span class=line><span class=cl>    <span class=c1># 5. Tests and logging.</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=c1># ...</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>X</span><span class=p>,</span> <span class=n>Y</span> <span class=ow>in</span> <span class=n>val_dataloader</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>loss</span> <span class=o>=</span> <span class=n>process_batch</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>,</span> <span class=n>loss_fn</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>training</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;validation loss: </span><span class=si>{</span><span class=n>loss</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>sleep</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>For writing this out from nothing, one just needs to have an abstract sense of
what one is aiming to do. We&rsquo;ll be training some model with some kind of
gradient descent, and we might as well just use Adam; the objective is going to
be some loss function which we should be able to identify once we have
understood the problem we&rsquo;re trying to solve at a modeling level.
The &ldquo;thinking&rdquo; tasks at this level thus seem to be:</p><ol><li>What&rsquo;s the loss function?</li><li>What are the parameters I need to pass in? Here I&rsquo;ve already written out some
of the parameters we&rsquo;ll need: optimizer parameters, and some parameters
associated to the dataset and dataloader. We&rsquo;ll eventually need more dataset
parameters, possibly, as well as model parameters. In general, we might also
have parameters associated to logging or the loss function. These can be
added in as the other sections of the loop skeleton are filled out.</li><li>What &ldquo;work&rdquo; do I need to do in the main loop, encapsulated in
<code>process_batch</code> here? This function can take a pretty simple skeleton form:</li></ol><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>process_batch</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>,</span> <span class=n>loss_fn</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>training</span><span class=o>=</span><span class=kc>True</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>preds</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=n>loss_fn</span><span class=p>(</span><span class=n>preds</span><span class=p>,</span> <span class=n>Y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>training</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>loss</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p>We use the model to make predictions, then evaluate the loss. We accumulate
gradients with <code>backward</code>, then step the optimizer and zero gradients.</p><p><strong><em>NOTE</em></strong>: it&rsquo;s helpful here to add some asserts to debug your tensor shapes! Even on
this simple linear regression problem, I spent some amount of time debugging
a case where my predictions variable was <code>(B, 1)</code>-shaped but my targets variable
was <code>(B,)</code>-shaped and broadcasting messed up the loss.</p><h2 id=dataset-component>Dataset component<a hidden class=anchor aria-hidden=true href=#dataset-component>#</a></h2><p>The thing to remember here is just that a <code>Dataloader</code> (from <code>torch.utils.data</code>)
wraps a <code>Dataset</code> object (from the same module). For this simple problem, we can
create our own <code>Dataset</code> subclass (we&rsquo;ll do this momentarily). Remember the
following for the dataloader:</p><ul><li>Set the batch size here.</li><li>If we&rsquo;re using CUDA, we can add the argument <code>pin_memory=True</code> and also use
the <code>non_blocking=True</code> option in our <code>dataset.to(device)</code> calls that move
data to the GPU. This can help performance.</li><li>The other arguments to the dataloader are probably not essential.</li></ul><h3 id=implementing-a-dataset-for-our-random-linear-regression-model>Implementing a dataset for our random linear regression model<a hidden class=anchor aria-hidden=true href=#implementing-a-dataset-for-our-random-linear-regression-model>#</a></h3><p>Here&rsquo;s code for implementing the dataset, with &ldquo;thought organization&rdquo; to follow.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>RandomDataset</span><span class=p>(</span><span class=n>Dataset</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>data_dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>num_samples</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>ground_truth</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span> <span class=o>|</span> <span class=kc>None</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>device</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s2>&#34;cpu&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=n>noise_std</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>transform</span><span class=p>:</span> <span class=n>Callable</span> <span class=o>|</span> <span class=kc>None</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=c1># target_transform: Callable | None = None,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>device</span> <span class=o>=</span> <span class=n>device</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>data</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=p>(</span><span class=n>num_samples</span><span class=p>,</span> <span class=n>data_dim</span><span class=p>),</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>noise_std</span> <span class=o>=</span> <span class=n>noise_std</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>ground_truth</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>ground_truth</span> <span class=o>=</span> <span class=n>ground_truth</span>  <span class=c1># can&#39;t clone a pytorch module; need to deepcopy or manually</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>ground_truth</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>data_dim</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>normal_</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>ground_truth</span><span class=o>.</span><span class=n>weight</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>zeros_</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>ground_truth</span><span class=o>.</span><span class=n>bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>parameter</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>ground_truth</span><span class=o>.</span><span class=n>parameters</span><span class=p>():</span>
</span></span><span class=line><span class=cl>                <span class=n>parameter</span><span class=o>.</span><span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>targets</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>ground_truth</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>data</span><span class=p>)</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>noise_std</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=p>(</span><span class=n>num_samples</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>transform</span> <span class=o>=</span> <span class=n>transform</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__len__</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__getitem__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>idx</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>        <span class=n>data</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>data</span><span class=p>[</span><span class=n>idx</span><span class=p>,</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>transform</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>data</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>targets</span><span class=p>[</span><span class=n>idx</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><p>High-level points:</p><ul><li>We subclass <code>torch.utils.data.Dataset</code> when we make our own dataset.</li><li>A <code>torch.utils.data.Dataset</code> needs to implement <code>__init__</code>, <code>__len__</code>, and
<code>__getitem__</code>. These are all instance methods; <code>__getitem__</code> also takes an
index (<code>0</code> to <code>__len__() - 1</code>) and returns a tuple containing the indexed
sample and its target/label. <em>These should each be <code>1D</code> tensors</em>.</li><li>For writing the code, we can sort of just write the <code>__init__</code> function and
think about what we need to set up for the data, then pretty easily do the
other two functions.</li><li>Set device and dtype for things.</li><li>To make things comparable, even though it&rsquo;s most natural to implement the
model&rsquo;s ground truth parameters with inline matrix operations, I used
a <code>torch.nn.Linear</code> layer here (which will parallel what we do with the model
below). For this, the arguments are <code>(in_features, out_features, ...)</code>. For
initialization, we use <code>torch.nn.init</code>; functions typically have a signature
like <code>..._</code>, and we just apply them to the module&rsquo;s parameters. The linear
layer&rsquo;s parameters are called <code>weight</code> and <code>bias</code>. We also set the parameters&rsquo;
<code>requires_grad</code> to false, and include <code>torch.no_grad()</code> context when we
generate the targets (ChatGPT says this tells pytorch not to track derived
operations in the computational graph and saves memory).</li><li>I&rsquo;m including transform (for normalization and data augmentation) here just to
be consistent with future stuff; will need this for more complicated
implementations (e.g., CIFAR10).</li></ul><h2 id=the-model>The model<a hidden class=anchor aria-hidden=true href=#the-model>#</a></h2><p>Here&rsquo;s the model code, which is already familiar because of how we generated our
dataset above.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>LinearModel</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span> <span class=n>data_dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>device</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s2>&#34;cpu&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>linear</span> <span class=o>=</span> <span class=n>Linear</span><span class=p>(</span><span class=n>data_dim</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>normal_</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>linear</span><span class=o>.</span><span class=n>weight</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>zeros_</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>linear</span><span class=o>.</span><span class=n>bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>linear</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Some short notes (reference the above):</p><ul><li>A pytorch model subclasses <code>torch.nn.Module</code>. It needs to implement
<code>__init__</code>, where we set up its parameters and structure, and <code>forward</code>, an
instance method that takes an input <code>x</code> and generates the layer output from
it.</li></ul><h2 id=putting-it-together>Putting it together<a hidden class=anchor aria-hidden=true href=#putting-it-together>#</a></h2><p>Here&rsquo;s our overall train loop, callable from the command line.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span><span class=lnt>72
</span><span class=lnt>73
</span><span class=lnt>74
</span><span class=lnt>75
</span><span class=lnt>76
</span><span class=lnt>77
</span><span class=lnt>78
</span><span class=lnt>79
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>train</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>data_dim</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>num_samples_train</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>num_samples_val</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>device</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>noise_std</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>batch_size</span><span class=o>=</span><span class=mi>32</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>num_epochs</span><span class=o>=</span><span class=mi>128</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=o>**</span><span class=n>kwargs</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 1. logging (wandb)</span>
</span></span><span class=line><span class=cl>    <span class=c1># We could do this with hydra. Chatgpt recommended a kwargs dict structure but hydra is better. Too much rewriting config table.</span>
</span></span><span class=line><span class=cl>    <span class=c1># 2. get data</span>
</span></span><span class=line><span class=cl>    <span class=n>train_dataset</span> <span class=o>=</span> <span class=n>RandomDataset</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>data_dim</span><span class=o>=</span><span class=n>data_dim</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>num_samples</span><span class=o>=</span><span class=n>num_samples_train</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>noise_std</span><span class=o>=</span><span class=n>noise_std</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>train_dataloader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>train_dataset</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>drop_last</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>val_dataset</span> <span class=o>=</span> <span class=n>RandomDataset</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>data_dim</span><span class=o>=</span><span class=n>data_dim</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>num_samples</span><span class=o>=</span><span class=n>num_samples_val</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>noise_std</span><span class=o>=</span><span class=n>noise_std</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>ground_truth</span><span class=o>=</span><span class=n>train_dataset</span><span class=o>.</span><span class=n>ground_truth</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>val_dataloader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>val_dataset</span><span class=p>,</span> <span class=n>drop_last</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=n>num_samples_val</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># 3. get model</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>LinearModel</span><span class=p>(</span><span class=n>data_dim</span><span class=p>,</span> <span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># 4. train</span>
</span></span><span class=line><span class=cl>    <span class=n>loss_fn</span> <span class=o>=</span> <span class=n>MSELoss</span><span class=p>(</span><span class=n>reduction</span><span class=o>=</span><span class=s2>&#34;mean&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>AdamW</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>1e-1</span><span class=p>,</span> <span class=n>weight_decay</span><span class=o>=</span><span class=mf>0.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>batch</span><span class=p>,</span> <span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>train_dataloader</span>
</span></span><span class=line><span class=cl>        <span class=p>):</span>  <span class=c1># enumerate(iterable) gives us the batch idx.</span>
</span></span><span class=line><span class=cl>            <span class=n>loss</span> <span class=o>=</span> <span class=n>process_batch</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>,</span> <span class=n>loss_fn</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;batch </span><span class=si>{</span><span class=n>batch</span><span class=si>}</span><span class=s2>, epoch </span><span class=si>{</span><span class=n>epoch</span><span class=si>}</span><span class=s2>, loss: </span><span class=si>{</span><span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># 5. Tests and logging.</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=sa>f</span><span class=s2>&#34;parameter error: </span><span class=si>{</span><span class=n>torch</span><span class=o>.</span><span class=n>sum</span><span class=p>((</span><span class=n>model</span><span class=o>.</span><span class=n>linear</span><span class=o>.</span><span class=n>weight</span> <span class=o>-</span> <span class=n>train_dataset</span><span class=o>.</span><span class=n>ground_truth</span><span class=o>.</span><span class=n>weight</span><span class=p>)</span><span class=o>**</span><span class=mi>2</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>X</span><span class=p>,</span> <span class=n>Y</span> <span class=ow>in</span> <span class=n>val_dataloader</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>loss</span> <span class=o>=</span> <span class=n>process_batch</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>,</span> <span class=n>loss_fn</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>training</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;validation loss: </span><span class=si>{</span><span class=n>loss</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>sleep</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>process_batch</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>,</span> <span class=n>loss_fn</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>training</span><span class=o>=</span><span class=kc>True</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>preds</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=n>loss_fn</span><span class=p>(</span><span class=n>preds</span><span class=p>,</span> <span class=n>Y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>training</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>loss</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>num_samples_train</span> <span class=o>=</span> <span class=mi>128</span>
</span></span><span class=line><span class=cl>    <span class=n>config</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;data_dim&#34;</span><span class=p>:</span> <span class=mi>16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;num_samples_train&#34;</span><span class=p>:</span> <span class=n>num_samples_train</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;num_samples_val&#34;</span><span class=p>:</span> <span class=mi>128</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;noise_std&#34;</span><span class=p>:</span> <span class=mf>0.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;batch_size&#34;</span><span class=p>:</span> <span class=n>num_samples_train</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;num_epochs&#34;</span><span class=p>:</span> <span class=mi>2</span><span class=o>**</span><span class=mi>10</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;device&#34;</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s2>&#34;cpu&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=n>train</span><span class=p>(</span><span class=o>**</span><span class=n>config</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>We can write all this code in a bottom-up format without losing too much
momentum!</p><h1 id=some-concluding-thoughts>Some concluding thoughts<a hidden class=anchor aria-hidden=true href=#some-concluding-thoughts>#</a></h1><ul><li>I didn&rsquo;t include much logging code above. It would be best to do this with
wandb and hydra, but I guess there are not parameters to do this in an
interview.</li><li>The code above is not particularly optimized. ChatGPT gives some
recommendations about enabling mixed-precision training (seems to be mild
overhead) and implementing incremental logging. It would be fun to implement
this exercise in a distributed training setting, as well.</li><li>For doing more complex stuff, it probably makes sense to think about a way to
write incremental tests (e.g., test the dataset is doing the right thing, test
the model is doing the right thing) as one goes. Otherwise when something
breaks it is a little bit annoying to drill down on what it is in this kind of
&ldquo;bottom up&rdquo; implementation scheme.</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/research/>Research</a></li></ul><nav class=paginav><a class=next href=http://localhost:1313/posts/first/><span class=title>Next ¬ª</span><br><span>Intro</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>hwagyesa</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>