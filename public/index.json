[{"content":"Rationale I don\u0026rsquo;t think I\u0026rsquo;m that bad at coding, but I\u0026rsquo;m certainly much slower than I\u0026rsquo;d like to be. I did not really learn pytorch until 2022, using a mixture of adhoc numpy and fast (but who cares) Matlab before that. For preparing for random ML coding questions in interviews (and, in some useful sense, I think, improving my general ability!), I\u0026rsquo;m trying to be a bit strategic and deliberate in my preparation. In this post I\u0026rsquo;ll record some notes for my own benefit on how a bog-standard data-train-eval loop in pytorch can be blocked up into recognizable segments and rolled out quickly from scratch. I am sure this is uninteresting to every good ML experimenter out there; in writing it out this way for myself, I\u0026rsquo;m hoping it will be a useful preparation reference, as well as something that reduces my barrier to quick experimentation in new problem areas ðŸ˜„\nThe exercise: linear regression We\u0026rsquo;ll spin this out implementing an extremely simple linear regression problem in pytorch. Something like the following data model: $$ \\boldsymbol y = \\boldsymbol X \\boldsymbol \\beta_o + \\sigma \\boldsymbol g,$$ with $n$ observations, $d$ dimensions, noise standard deviation $\\sigma \u003e 0$, and everything i.i.d. $\\mathcal N(0, 1)$. For this data model, we will:\nCode up the training loop. Code up the dataset. Code up the model. Add in eval (with wandb; maybe Hydra). I\u0026rsquo;ll put notes on these sub-components in the sections below.\nTraining loop For interview coding, it seems to make sense to start by writing out the \u0026lsquo;skeleton\u0026rsquo; of the training loop, then filling in the implementation-specific details: the model, the data, and different eval components necessary. This seems like a good structure to use as long as one has enough familiarity with pytorch to know how to start abstractly.\nThe following basic structure makes sense to me as a starting point:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def train( learning_rate = 1e-3, weight_decay = 0., batch_size = 32, num_samples_val = ..., **kwargs ): # TODO: # 1. logging (wandb) # ... # 2. get data train_dataloader = DataLoader( train_dataset, batch_size=batch_size, drop_last=False, shuffle=True ) val_dataloader = DataLoader( val_dataset, drop_last=False, shuffle=True, batch_size=num_samples_val ) # 3. get model model = ... # 4. training loop: loss function, optimizer, epoch and batch loop, batch processing. loss_fn = MSELoss(reduction=\u0026#34;mean\u0026#34;) optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay) model.train() for epoch in range(num_epochs): for batch, (X, Y) in enumerate( train_dataloader ): loss = process_batch(model, X, Y, loss_fn, optimizer) # log stuff to wandb. # 5. Tests and logging. model.eval() # ... with torch.no_grad(): for X, Y in val_dataloader: loss = process_batch(model, X, Y, loss_fn, optimizer, training=False) print(f\u0026#34;validation loss: {loss.detach()}\u0026#34;) sleep(1) For writing this out from nothing, one just needs to have an abstract sense of what one is aiming to do. We\u0026rsquo;ll be training some model with some kind of gradient descent, and we might as well just use Adam; the objective is going to be some loss function which we should be able to identify once we have understood the problem we\u0026rsquo;re trying to solve at a modeling level. The \u0026ldquo;thinking\u0026rdquo; tasks at this level thus seem to be:\nWhat\u0026rsquo;s the loss function? What are the parameters I need to pass in? Here I\u0026rsquo;ve already written out some of the parameters we\u0026rsquo;ll need: optimizer parameters, and some parameters associated to the dataset and dataloader. We\u0026rsquo;ll eventually need more dataset parameters, possibly, as well as model parameters. In general, we might also have parameters associated to logging or the loss function. These can be added in as the other sections of the loop skeleton are filled out. What \u0026ldquo;work\u0026rdquo; do I need to do in the main loop, encapsulated in process_batch here? This function can take a pretty simple skeleton form: 1 2 3 4 5 6 7 8 9 def process_batch(model, X, Y, loss_fn, optimizer, training=True): preds = model(X) loss = loss_fn(preds, Y) if training: loss.backward() optimizer.step() optimizer.zero_grad() return loss.detach() We use the model to make predictions, then evaluate the loss. We accumulate gradients with backward, then step the optimizer and zero gradients.\nNOTE: it\u0026rsquo;s helpful here to add some assets to debug your tensor shapes! Even on this simple linear regression problem, I spent some amount of time debugging a case where my predictions variable was (B, 1)-shaped but my targets variable was (B,)-shaped and broadcasting messed up the loss.\nDataset component The thing to remember here is just that a Dataloader (from torch.utils.data) wraps a Dataset object (from the same module). For this simple problem, we can create our own Dataset subclass (we\u0026rsquo;ll do this momentarily). Remember the following for the dataloader:\nSet the batch size here. If we\u0026rsquo;re using CUDA, we can add the argument pin_memory=True and also use the non_blocking=True option in our dataset.to(device) calls that move data to the GPU. This can help performance. The other arguments to the dataloader are probably not essential. Implementing a dataset for our random linear regression model Here\u0026rsquo;s code for implementing the dataset, with \u0026ldquo;thought organization\u0026rdquo; to follow.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class RandomDataset(Dataset): def __init__( self, data_dim: int, num_samples: int, ground_truth: torch.nn.Linear | None = None, device: torch.device = torch.device(\u0026#34;cpu\u0026#34;), noise_std: float = 0.1, transform: Callable | None = None, # target_transform: Callable | None = None, ) -\u0026gt; None: super().__init__() self.device = device self.data = torch.randn( (num_samples, data_dim), device=device, dtype=torch.float32 ) self.noise_std = noise_std if ground_truth is not None: self.ground_truth = ground_truth # can\u0026#39;t clone a pytorch module; need to deepcopy or manually else: self.ground_truth = torch.nn.Linear( data_dim, 1, device=device, dtype=torch.float32 ) torch.nn.init.normal_(self.ground_truth.weight) torch.nn.init.zeros_(self.ground_truth.bias) for parameter in self.ground_truth.parameters(): parameter.requires_grad = False with torch.no_grad(): self.targets = self.ground_truth(self.data) + self.noise_std * torch.randn( (num_samples, 1), device=device, dtype=torch.float32 ) self.transform = transform def __len__(self) -\u0026gt; int: return self.data.shape[0] def __getitem__(self, idx) -\u0026gt; tuple[torch.Tensor, torch.Tensor]: data = self.data[idx, :] if self.transform is not None: data = self.transform(data) return (data, self.targets[idx]) High-level points:\nWe subclass torch.utils.data.Dataset when we make our own dataset. A torch.utils.data.Dataset needs to implement __init__, __len__, and __getitem__. These are all instance methods; __getitem__ also takes an index (0 to __len__() - 1) and returns a tuple containing the indexed sample and its target/label. These should each be 1D tensors. For writing the code, we can sort of just write the __init__ function and think about what we need to set up for the data, then pretty easily do the other two functions. Set device and dtype for things. To make things comparable, even though it\u0026rsquo;s most natural to implement the model\u0026rsquo;s ground truth parameters with inline matrix operations, I used a torch.nn.Linear layer here (which will parallel what we do with the model below). For this, the arguments are (in_features, out_features, ...). For initialization, we use torch.nn.init; functions typically have a signature like ..._, and we just apply them to the module\u0026rsquo;s parameters. The linear layer\u0026rsquo;s parameters are called weight and bias. We also set the parameters\u0026rsquo; requires_grad to false, and include torch.no_grad() context when we generate the targets (ChatGPT says this tells pytorch not to track derived operations in the computational graph and saves memory). I\u0026rsquo;m including transform (for normalization and data augmentation) here just to be consistent with future stuff; will need this for more complicated implementations (e.g., CIFAR10). The model Here\u0026rsquo;s the model code, which is already familiar because of how we generated our dataset above.\n1 2 3 4 5 6 7 8 9 10 11 12 class LinearModel(torch.nn.Module): def __init__( self, data_dim: int, device: torch.device = torch.device(\u0026#34;cpu\u0026#34;) ) -\u0026gt; None: super().__init__() self.linear = Linear(data_dim, 1, device=device, dtype=torch.float32) torch.nn.init.normal_(self.linear.weight) torch.nn.init.zeros_(self.linear.bias) def forward(self, x) -\u0026gt; torch.Tensor: return self.linear(x) Some short notes (reference the above):\nA pytorch model subclasses torch.nn.Module. It needs to implement __init__, where we set up its parameters and structure, and forward, an instance method that takes an input x and generates the layer output from it. Putting it together Here\u0026rsquo;s our overall train loop, callable from the command line.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def train( data_dim, num_samples_train, num_samples_val, device, noise_std=0.1, batch_size=32, num_epochs=128, **kwargs, ): # 1. logging (wandb) # We could do this with hydra. Chatgpt recommended a kwargs dict structure but hydra is better. Too much rewriting config table. # 2. get data train_dataset = RandomDataset( data_dim=data_dim, num_samples=num_samples_train, noise_std=noise_std, device=device, ) train_dataloader = DataLoader( train_dataset, batch_size=batch_size, drop_last=False, shuffle=True ) val_dataset = RandomDataset( data_dim=data_dim, num_samples=num_samples_val, noise_std=noise_std, ground_truth=train_dataset.ground_truth, ) val_dataloader = DataLoader( val_dataset, drop_last=False, shuffle=True, batch_size=num_samples_val ) # 3. get model model = LinearModel(data_dim, device) # 4. train loss_fn = MSELoss(reduction=\u0026#34;mean\u0026#34;) optimizer = torch.optim.AdamW(model.parameters(), lr=1e-1, weight_decay=0.0) model.train() for epoch in range(num_epochs): for batch, (X, Y) in enumerate( train_dataloader ): # enumerate(iterable) gives us the batch idx. loss = process_batch(model, X, Y, loss_fn, optimizer) print(f\u0026#34;batch {batch}, epoch {epoch}, loss: {loss.item()}\u0026#34;) # 5. Tests and logging. model.eval() print( f\u0026#34;parameter error: {torch.sum((model.linear.weight - train_dataset.ground_truth.weight)**2)}\u0026#34; ) with torch.no_grad(): for X, Y in val_dataloader: loss = process_batch(model, X, Y, loss_fn, optimizer, training=False) print(f\u0026#34;validation loss: {loss.detach()}\u0026#34;) sleep(1) def process_batch(model, X, Y, loss_fn, optimizer, training=True): preds = model(X) loss = loss_fn(preds, Y) if training: loss.backward() optimizer.step() optimizer.zero_grad() return loss.detach() if __name__ == \u0026#34;__main__\u0026#34;: num_samples_train = 128 config = { \u0026#34;data_dim\u0026#34;: 16, \u0026#34;num_samples_train\u0026#34;: num_samples_train, \u0026#34;num_samples_val\u0026#34;: 128, \u0026#34;noise_std\u0026#34;: 0.0, \u0026#34;batch_size\u0026#34;: num_samples_train, \u0026#34;num_epochs\u0026#34;: 2**10, \u0026#34;device\u0026#34;: torch.device(\u0026#34;cpu\u0026#34;), } train(**config) We can write all this code in a bottom-up format without losing too much momentum!\nSome concluding thoughts I didn\u0026rsquo;t include much logging code above. It would be best to do this with wandb and hydra, but I guess there are not parameters to do this in an interview. The code above is not particularly optimized. ChatGPT gives some recommendations about enabling mixed-precision training (seems to be mild overhead) and implementing incremental logging. It would be fun to implement this exercise in a distributed training setting, as well. For doing more complex stuff, it probably makes sense to think about a way to write incremental tests (e.g., test the dataset is doing the right thing, test the model is doing the right thing) as one goes. Otherwise when something breaks it is a little bit annoying to drill down on what it is in this kind of \u0026ldquo;bottom up\u0026rdquo; implementation scheme. ","permalink":"http://localhost:1313/posts/pytorch-speedrunning/","summary":"\u003ch1 id=\"rationale\"\u003eRationale\u003c/h1\u003e\n\u003cp\u003eI don\u0026rsquo;t think I\u0026rsquo;m \u003cem\u003ethat\u003c/em\u003e bad at coding, but I\u0026rsquo;m certainly much slower than I\u0026rsquo;d\nlike to be. I did not really learn pytorch until 2022, using a mixture of adhoc\nnumpy and fast (but who cares) Matlab before that. For preparing for random ML\ncoding questions in interviews (and, in some useful sense, I think, improving my\ngeneral ability!), I\u0026rsquo;m trying to be a bit strategic and deliberate in my preparation.\nIn this post I\u0026rsquo;ll record some notes for my own benefit on how a bog-standard\ndata-train-eval loop in pytorch can be blocked up into recognizable segments and\nrolled out quickly from scratch. I am sure this is uninteresting to every good\nML experimenter out there; in writing it out this way for myself, I\u0026rsquo;m hoping it\nwill be a useful preparation reference, as well as something that reduces my\nbarrier to quick experimentation in new problem areas ðŸ˜„\u003c/p\u003e","title":"Pytorch Speedrunning"},{"content":"After years of doing theory1 , I\u0026rsquo;m trying to transition into doing applied research. The first step is to pass industry research position interviews. In the short term, I\u0026rsquo;ll record some notes on this blog relevant to my interview prep \u0026ndash; for example, basic statistical decision theory, reinforcement learning, certain simple model implementations, basic MLsys, etc. (things that have been \u0026ldquo;slippery\u0026rdquo; for me so far, but are not really complicated).\nIn the future, if I am so fortunate, maybe I\u0026rsquo;ll be able to start recording research ideas on this blog. Stay tuned!\nWith diminishing returns, probably exponentially-diminishing after 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/first/","summary":"\u003cp\u003eAfter years of doing theory\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e , I\u0026rsquo;m trying to transition\ninto doing applied research. The first step is to pass industry research\nposition interviews. In the short term, I\u0026rsquo;ll record some notes on this blog\nrelevant to my interview prep \u0026ndash; for example, basic statistical decision theory,\nreinforcement learning, certain simple model implementations, basic MLsys, etc.\n(things that have been \u0026ldquo;slippery\u0026rdquo; for me so far, but are not really\ncomplicated).\u003c/p\u003e","title":"Intro"}]